![OM_Banner_X2](https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c)

<p align="center">
  <a href="https://arxiv.org/abs/2412.18588">Technical Paper</a> |
  <a href="https://docs.openmind.org/">Documentation</a> |
  <a href="https://x.com/openmind_agi">X</a> |
  <a href="https://discord.gg/VUjpg4ef5n">Discord</a>
</p>

---

**OpenMind's OM1** is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots, including humanoids, phone apps, websites, quadrupeds, and educational robots such as TurtleBot 4.  
OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversation.  

The goal of OM1 is to make it easy to create highly capable human-focused robots that are easy to upgrade and (re)configure to accommodate different physical form factors.

---

## Capabilities of OM1

* **Modular Architecture**: Designed with Python for simplicity and seamless integration.  
* **Data Input**: Easily handles new data and sensors.  
* **Hardware Support via Plugins**: Supports new hardware through plugins for API endpoints and specific connections to `ROS2` and `CycloneDDS`. (We recommend ROS2 for new development.)